{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Input Embeddings**: Token + Positional embeddings for each token\n",
    "- **Contextual Embeddings**: Embedding after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive loss is a type of loss function used in machine learning, particularly in tasks involving similarity learning, such as in Siamese networks. It works by comparing pairs of samples and adjusting their representations based on their similarity:\n",
    "\n",
    "- **For similar pairs**: The loss function minimizes the distance between their embeddings.\n",
    "- **For dissimilar pairs**: The loss function maximizes the distance between their embeddings, often with a margin to prevent overlap¹(https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/)²(https://arxiv.org/abs/2012.09740).\n",
    "\n",
    "This approach helps the model learn to distinguish between similar and dissimilar samples effectively³(https://codelabsacademy.com/blog/understanding-contrastive-loss-and-reconstruction-loss-in-machine-learning)⁴(https://www.baeldung.com/cs/contrastive-learning).\n",
    "\n",
    "Is there a specific application or example of contrastive loss you're curious about?\n",
    "\n",
    "Source: Conversation with Copilot, 2024/9/2\n",
    "(1) Contrastive Loss for Siamese Networks with Keras and TensorFlow. https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/.\n",
    "(2) [2012.09740] Understanding the Behaviour of Contrastive Loss - arXiv.org. https://arxiv.org/abs/2012.09740.\n",
    "(3) Understanding Contrastive Loss and Reconstruction Loss in Machine .... https://codelabsacademy.com/blog/understanding-contrastive-loss-and-reconstruction-loss-in-machine-learning.\n",
    "(4) An Introduction to Contrastive Learning | Baeldung on Computer Science. https://www.baeldung.com/cs/contrastive-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive loss is a type of loss function used in machine learning, particularly in tasks involving similarity learning, such as in Siamese networks. It works by comparing pairs of samples and adjusting their representations based on their similarity:\n",
    "\n",
    "- **For similar pairs**: The loss function minimizes the distance between their embeddings.\n",
    "- **For dissimilar pairs**: The loss function maximizes the distance between their embeddings, often with a margin to prevent overlap¹(https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/)²(https://arxiv.org/abs/2012.09740).\n",
    "\n",
    "This approach helps the model learn to distinguish between similar and dissimilar samples effectively³(https://codelabsacademy.com/blog/understanding-contrastive-loss-and-reconstruction-loss-in-machine-learning)⁴(https://www.baeldung.com/cs/contrastive-learning).\n",
    "\n",
    "Is there a specific application or example of contrastive loss you're curious about?\n",
    "\n",
    "Source: Conversation with Copilot, 2024/9/2\n",
    "(1) Contrastive Loss for Siamese Networks with Keras and TensorFlow. https://pyimagesearch.com/2021/01/18/contrastive-loss-for-siamese-networks-with-keras-and-tensorflow/.\n",
    "(2) [2012.09740] Understanding the Behaviour of Contrastive Loss - arXiv.org. https://arxiv.org/abs/2012.09740.\n",
    "(3) Understanding Contrastive Loss and Reconstruction Loss in Machine .... https://codelabsacademy.com/blog/understanding-contrastive-loss-and-reconstruction-loss-in-machine-learning.\n",
    "(4) An Introduction to Contrastive Learning | Baeldung on Computer Science. https://www.baeldung.com/cs/contrastive-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [4.3, 1.2, 0.05, 1.07],\n",
    "        [0.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 1.03],\n",
    "        [0.23, 0.57, 0.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(data):\n",
    "    target = torch.arange(data.size(0))\n",
    "    loss = torch.nn.CrossEntropyLoss()(data, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9100, 0.0410, 0.0130, 0.0360],\n",
       "        [0.0429, 0.8801, 0.0393, 0.0377],\n",
       "        [0.1512, 0.0846, 0.5832, 0.1810],\n",
       "        [0.0075, 0.0105, 0.0067, 0.9753]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3000, 1.2000, 0.0500, 1.0700],\n",
      "        [0.1800, 3.2000, 0.0900, 0.0500],\n",
      "        [0.8500, 0.2700, 2.2000, 1.0300],\n",
      "        [0.2300, 0.5700, 0.1200, 5.1000]])\n",
      "Loss = 0.19657586514949799\n",
      "tensor([[4.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [0.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 1.0100],\n",
      "        [0.2100, 0.5500, 0.1000, 5.6000]])\n",
      "Loss = 0.12602083384990692\n",
      "tensor([[5.3000, 1.1600, 0.0100, 1.0300],\n",
      "        [0.1400, 4.2000, 0.0500, 0.0100],\n",
      "        [0.8100, 0.2300, 3.2000, 0.9900],\n",
      "        [0.1900, 0.5300, 0.0800, 6.1000]])\n",
      "Loss = 0.07888662070035934\n"
     ]
    }
   ],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "\n",
    "for inx in range(3):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5\n",
    "    data[non_diag_mask] -= inx*0.02\n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                embed_dim,\n",
    "                nhead=8,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=3,\n",
    "            norm=torch.nn.LayerNorm(embed_dim),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "\n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask=tokenizer_output['attention_mask'])\n",
    "        cls_embed = x[:, 0, :]\n",
    "        return self.projection(cls_embed)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataset):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()),\n",
    "        lr=1e-5\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    running_loss = []\n",
    "    for _, data_batch in enumerate(dataloader):\n",
    "        # Tokenize the question/answer pairs (each is batch of 32 questions and 32 answers)\n",
    "        question, answer = data_batch\n",
    "        question_tok = tokenizer(question, padding=True, truncation=True, max_length=max_seq_len, return_tensors='pt')\n",
    "        answer_tok = tokenizer(answer, padding=True, truncation=True, max_length=max_seq_len, return_tensors='pt')\n",
    "\n",
    "        # Compute the embeddings: the output is of dim = 32 x 128\n",
    "        question_embed = question_encoder(question_tok)\n",
    "        answer_embed = answer_encoder(answer_tok)\n",
    "\n",
    "        # Compute similarity scores: a 32x32 matrix\n",
    "        # row[N] reflects similarity between question[N] and all answers\n",
    "        similarity_scores = question_embed @ answer_embed.T\n",
    "\n",
    "        # We want to maximize the values in the diagonal and minimize the rest\n",
    "        target = torch.arrange(question_embed.shape[0], dtype=torch.long)\n",
    "        loss = loss_fn(similarity_scores, target)\n",
    "        running_loss += [loss.item()]\n",
    "\n",
    "        # This is where the magic happens\n",
    "        optimizer.zero_grad() # reset optimizer so gradients are all-zero\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, num_epochs=10):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "\n",
    "    # Define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # Define the dataloader, optimizer and loss function\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()),\n",
    "        lr=1e-5\n",
    "    )\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for inx, data_batch in enumerate(dataloader):\n",
    "            # Tokenize the question/answer pairs (each is batch of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, max_length=max_seq_len, return_tensors='pt')\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, max_length=max_seq_len, return_tensors='pt')\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)                \n",
    "\n",
    "            # Compute the embeddings: the output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "\n",
    "            # Compute similarity scores: a 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and all answers\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "\n",
    "            # We want to maximize the values in the diagonal and minimize the rest\n",
    "            target = torch.arrange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if inx == n_iters - 1:\n",
    "                print(f\"Epoch {epoch}, loss =\", np.mean(running_loss))\n",
    "                      \n",
    "            # This is where the magic happens\n",
    "            optimizer.zero_grad() # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep='\\t'), nrows=300)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['question'], self.data.iloc[idx]['answer']\n",
    "    \n",
    "dataset = MyDataset('../shared_data/nq_sample.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe, ae = train(dataset, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the tallest mountain in the world?\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "question_embed = qe(question_tok)[0]\n",
    "print(question_tok)\n",
    "print(question_embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\",\n",
    "]\n",
    "\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
