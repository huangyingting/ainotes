{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and querying for embeddings with Redis\n",
    "\n",
    "Here’s the rough architecture:\n",
    "![Architecture](images/redsis.png)\n",
    "\n",
    "The steps above do the following:\n",
    "\n",
    "- A console app retrieves blog post URLs from an RSS feed and reads all the posts one by one\n",
    "- For each post, create an embedding with OpenAI which results in a vector of 1536 dimensions to store in redis\n",
    "- After the embedding is created, store the embedding in a redis search index, we create this index by using redis python client\n",
    "- Perform a vectorized search, finding the closest post vectors to the query vector using HNSW algorithm\n",
    "\n",
    "To start with, we need to run redis stack on docker, to get started with the redis stack, run the following command in your terminal:\n",
    "\n",
    "`docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -U openai redis feedparser numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing post data in Redis hashes\n",
    "\n",
    "We will create several Redis hashes, one for each post. Hashes are records structured as collections of field-value pairs. Each hash we store, has the following fields:\n",
    "\n",
    "- url: url to the blog post\n",
    "- embedding: embedding of the blog post (a vector), created with the OpenAI embeddings API and the text-embedding-ada-002 model\n",
    "\n",
    "We need the URL to retrieve the entire post after a closest match has been found. In Pinecone, the URL would be metadata to the vector. In Redis, it’s just a field in a hash, just like the vector itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries:  10\n",
      "Create embedding and save for entry  0  of  10\n",
      "Create embedding and save for entry  1  of  10\n",
      "Create embedding and save for entry  2  of  10\n",
      "Create embedding and save for entry  3  of  10\n",
      "Create embedding and save for entry  4  of  10\n",
      "Create embedding and save for entry  5  of  10\n",
      "Create embedding and save for entry  6  of  10\n",
      "Create embedding and save for entry  7  of  10\n",
      "Create embedding and save for entry  8  of  10\n",
      "Create embedding and save for entry  9  of  10\n",
      "Vector upload complete.\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "import redis\n",
    "from redis.commands.search.field import VectorField, TextField\n",
    "from redis.commands.search.query import Query\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Redis connection details\n",
    "redis_host = os.getenv('REDIS_HOST')\n",
    "redis_port = os.getenv('REDIS_PORT')\n",
    "redis_password = os.getenv('REDIS_PASSWORD')\n",
    "\n",
    "# Connect to the Redis server\n",
    "conn = redis.Redis(host=os.getenv('REDIS_HOST'), \n",
    "                   port=os.getenv('REDIS_PORT'), \n",
    "                   password=os.getenv('REDIS_PASSWORD'))\n",
    "client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "SCHEMA = [\n",
    "    TextField(\"url\"),\n",
    "    VectorField(\"embedding\", \"HNSW\", {\"TYPE\": \"FLOAT32\", \"DIM\": 1536, \"DISTANCE_METRIC\": \"COSINE\"}),\n",
    "]\n",
    "\n",
    "# Create the index\n",
    "try:\n",
    "    conn.ft(\"posts\").create_index(fields=SCHEMA, definition=IndexDefinition(prefix=[\"post:\"], index_type=IndexType.HASH))\n",
    "except Exception as e:\n",
    "    print(\"Index already exists\")\n",
    "\n",
    "# URL of the RSS feed to parse\n",
    "url = 'https://devblogs.microsoft.com/landingpage/'\n",
    "\n",
    "# Parse the RSS feed with feedparser\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "# get number of entries in feed\n",
    "entries = len(feed.entries)\n",
    "print(\"Number of entries: \", entries)\n",
    "\n",
    "p = conn.pipeline(transaction=False)\n",
    "for i, entry in enumerate(feed.entries[:50]):\n",
    "    # report progress\n",
    "    print(\"Create embedding and save for entry \", i, \" of \", entries)\n",
    "    \n",
    "    article = entry.description\n",
    "\n",
    "    embedding = client.embeddings.create(\n",
    "        input=article,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "    # print the embedding (length = 1536)\n",
    "    vector = embedding.data[0].embedding\n",
    "\n",
    "    # convert to numpy array\n",
    "    vector = np.array(vector).astype(np.float32).tobytes()\n",
    "\n",
    "    # Create a new hash with the URL and embedding\n",
    "    post_hash = {\n",
    "        \"url\": entry.link,\n",
    "        \"embedding\": vector\n",
    "    }\n",
    "    \n",
    "    # add_document() is deprecated\n",
    "    conn.hset(name=f\"post:{i}\", mapping=post_hash)\n",
    "\n",
    "p.execute()\n",
    "\n",
    "print(\"Vector upload complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redis vector queries\n",
    "\n",
    "With the hashes and the index created, we can now perform a similarity search. We will ask the user for a query string (use natural language) and then check the posts that are similar to the query string. The query string will need to be vectorized as well. We will return several post and rank them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redis\n",
      "Vectorizing query...\n",
      "Searching for similar posts...\n",
      "Found 5 results:\n",
      "\t0. https://devblogs.microsoft.com/identity/eng-connect-jun-24 (Score: 0.825)\n",
      "\t1. https://devblogs.microsoft.com/visualstudio/automatically-install-visual-studio-security-updates-through-microsoft-update (Score: 0.816)\n",
      "\t2. https://devblogs.microsoft.com/directx/step-forward-for-gaming-on-arm-devices-2024 (Score: 0.81)\n",
      "\t3. https://devblogs.microsoft.com/qsharp/evaluating-cat-qubits-for-fault-tolerant-quantum-computing-using-azure-quantum-resource-estimator (Score: 0.796)\n",
      "\t4. https://devblogs.microsoft.com/ise/empowering-collaboration-with-tech-savvy-customer (Score: 0.785)\n"
     ]
    }
   ],
   "source": [
    "def search_vectors(query_vector, client, top_k=5):\n",
    "    base_query = \"*=>[KNN 5 @embedding $vector AS vector_score]\"\n",
    "    query = Query(base_query).return_fields(\"url\", \"vector_score\").sort_by(\"vector_score\").dialect(2)    \n",
    "\n",
    "    try:\n",
    "        results = client.ft(\"posts\").search(query, query_params={\"vector\": query_vector})\n",
    "    except Exception as e:\n",
    "        print(\"Error calling Redis search: \", e)\n",
    "        return None\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if conn.ping():\n",
    "    print(\"Connected to Redis\")\n",
    "\n",
    "# Enter a query\n",
    "query = \"Microsoft\"\n",
    "\n",
    "# Vectorize the query using OpenAI's text-embedding-ada-002 model\n",
    "print(\"Vectorizing query...\")\n",
    "embedding = client.embeddings.create(input=query, model=\"text-embedding-ada-002\")\n",
    "query_vector = embedding.data[0].embedding\n",
    "\n",
    "# Convert the vector to a numpy array\n",
    "query_vector = np.array(query_vector).astype(np.float32).tobytes()\n",
    "\n",
    "# Perform the similarity search\n",
    "print(\"Searching for similar posts...\")\n",
    "results = search_vectors(query_vector, conn)\n",
    "\n",
    "if results:\n",
    "    print(f\"Found {results.total} results:\")\n",
    "    for i, post in enumerate(results.docs):\n",
    "        score = 1 - float(post.vector_score)\n",
    "        print(f\"\\t{i}. {post.url} (Score: {round(score ,3) })\")\n",
    "else:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and querying for embeddings with Redis Vector Library (RedisVL)\n",
    "RedisVL provides a powerful, dedicated Python client library for using Redis as a [Vector Database](https://redis.com/solutions/use-cases/vector-database). Leverage the speed and reliability of Redis along with vector-based semantic search capabilities to supercharge your application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U redisvl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a schema\n",
    "\n",
    "Consider a dataset composed of 10k SEC filings PDFs, each broken down into manageable text chunks. Each record in this dataset includes:\n",
    "\n",
    "- Id: A unique identifier for each PDF chunk.\n",
    "- Content: The actual text extracted from the PDF.\n",
    "- Content Embedding: A vector representation of the section’s text.\n",
    "- Company: The name of the associated company.\n",
    "- Timestamp: A numeric value representing the last update time.\n",
    "\n",
    "First, define a schema that models this data’s structure in an index named `sec-filings`. Use a YAML file for convenience:\n",
    "\n",
    "```yaml\n",
    "index:\n",
    "  name: sec-filings\n",
    "  prefix: chunk\n",
    " \n",
    "fields:\n",
    "  - name: id\n",
    "    type: tag\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: content\n",
    "    type: text\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: company\n",
    "    type: tag\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: timestamp\n",
    "    type: numeric\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: content_embedding\n",
    "    type: vector\n",
    "    attrs:\n",
    "      dims: 1536\n",
    "      algorithm: hnsw\n",
    "      datatype: float32\n",
    "      distance_metric: cosine\n",
    "```\n",
    "Now, load and validate this schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.schema import IndexSchema\n",
    "import yaml\n",
    "\n",
    "yaml_schema = \"\"\"\n",
    "index:\n",
    "  name: sec-filings\n",
    "  prefix: chunk\n",
    " \n",
    "fields:\n",
    "  - name: id\n",
    "    type: tag\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: content\n",
    "    type: text\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: company\n",
    "    type: tag\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: timestamp\n",
    "    type: numeric\n",
    "    attrs:\n",
    "      sortable: true\n",
    "  - name: content_embedding\n",
    "    type: vector\n",
    "    attrs:\n",
    "      dims: 1536\n",
    "      algorithm: hnsw\n",
    "      datatype: float32\n",
    "      distance_metric: cosine\n",
    "\"\"\"\n",
    "\n",
    "json_schema = yaml.safe_load(yaml_schema)\n",
    "schema = IndexSchema.from_dict(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index\n",
    "Now we’ll create the index for our dataset by passing a Redis Python client connection to a `SearchIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from redisvl.index import SearchIndex\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Establish a connection with Redis\n",
    "conn = redis.Redis(host=os.getenv('REDIS_HOST'), \n",
    "                   port=os.getenv('REDIS_PORT'), \n",
    "                   password=os.getenv('REDIS_PASSWORD'))\n",
    " \n",
    "# Link the schema with our Redis client to create the search index\n",
    "index = SearchIndex(schema, conn)\n",
    " \n",
    "# Create the index in Redis\n",
    "index.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify embedding generation\n",
    "\n",
    "The [vectorizer](https://www.redisvl.com/user_guide/vectorizers_04.html) module provides access to popular embedding providers, below is an example using the Azure OpenAI vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.utils.vectorize import AzureOpenAITextVectorizer\n",
    "\n",
    "# create a vectorizer\n",
    "aoai = AzureOpenAITextVectorizer(\n",
    "    model=\"text-embedding-ada-002\", # Must be your CUSTOM deployment name\n",
    "    api_config={\n",
    "        \"api_key\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \"api_version\": os.getenv(\"OPENAI_API_VERSION\"),\n",
    "        \"azure_endpoint\":  os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    },\n",
    ")\n",
    " \n",
    "# Generate an embedding for a single query\n",
    "embedding = aoai.embed(\n",
    "    \"How much debt is the company in?\", input_type=\"search_query\"\n",
    ")\n",
    " \n",
    "# Generate embeddings for multiple queries\n",
    "embeddings = aoai.embed_many([\n",
    "    \"How much debt is the company in?\",\n",
    "    \"What do revenue projections look like?\"\n",
    "], input_type=\"search_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load data\n",
    "\n",
    "Before querying, use the vectorizer to create text embeddings and populate the index with your data. If your dataset is a collection of dictionary objects, the .load() method simplifies insertion. It batches upsert operations, efficiently storing your data in Redis and returning the keys for each record:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Example dataset as a list of dictionaries\n",
    "data = [\n",
    "    {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": \"Material Cybersecurity Incidents\",\n",
    "        \"company\": \"Microsoft\",\n",
    "        \"timestamp\": 20240127,\n",
    "        \"content_embedding\": aoai.embed(\n",
    "            \"As disclosed in the Original Filing, the Company detected that beginning in late November 2023, a nation-state threat actor had gained access to and exfiltrated information from a very small percentage of employee email accounts including members of our senior leadership team and employees in our cybersecurity, legal, and other functions.\",  input_type=\"search_document\", as_buffer=True\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": \"Cash Flows\",\n",
    "        \"company\": \"Microsoft\",\n",
    "        \"timestamp\": 20240130,\n",
    "        \"content_embedding\": aoai.embed(\n",
    "            \"Cash from operations increased $15.1 billion to $49.4 billion for the six months ended December 31, 2023, mainly due to an increase in cash received from customers and a decrease in cash paid to suppliers. Cash from financing increased $26.8 billion to $4.6 billion for the six months ended December 31, 2023, mainly due to a $25.4 billion increase in proceeds from issuance of debt, net of repayments. Cash used in investing increased $61.1 billion to $71.4 billion for the six months ended December 31, 2023, mainly due to a $65.2 billion increase in cash used for acquisitions of companies, net of cash acquired, and purchases of intangible and other assets.\",  input_type=\"search_document\", as_buffer=True\n",
    "        )\n",
    "    },    \n",
    "    # More records...\n",
    "]\n",
    " \n",
    "# Insert data into the index\n",
    "keys = index.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run queries\n",
    "\n",
    "The VectorQuery is a simple abstraction for performing KNN/ANN style vector searches with optional filters. \n",
    "\n",
    "Imagine you want to find the 5 PDF chunks most semantically related to a user’s query, such as \"What is the cash flow of this compnay\". First, convert the query into a vector using a text embedding model (see below section on vectorizers). Next, define and execute the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query import VectorQuery\n",
    " \n",
    "query = \"What is the cash flow of this company?\"\n",
    " \n",
    "query_vector = aoai.embed(query, input_type=\"search_query\", as_buffer=True)\n",
    " \n",
    "query = VectorQuery(\n",
    "    vector=query_vector, \n",
    "    vector_field_name=\"content_embedding\",\n",
    "    num_results=5\n",
    ")\n",
    " \n",
    "results = index.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further refine the search results, you can apply various metadata filters. For example, if you’re interested in documents specifically related to \"Microsoft\", use a Tag filter on the company field:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.query.filter import Tag\n",
    " \n",
    "# Apply a filter for the company name\n",
    "query.set_filter(Tag(\"company\") == \"Microsoft\")\n",
    " \n",
    "# Execute the filtered query\n",
    "results = index.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters allow you to combine searches over structured data (metadata) with vector similarity to improve retrieval precision.\n",
    "\n",
    "The VectorQuery is just the starting point. For those looking to explore more advanced querying techniques and data types (text, tag, numeric, vector, geo), [this dedicated user guide](https://www.redisvl.com/user_guide/hybrid_queries_02.html) will get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boost performance with semantic caching\n",
    "\n",
    "`redisvl` goes beyond facilitating vector search and query operations in Redis; it aims to showcase practical use cases and common LLM design patterns.\n",
    "\n",
    "[Semantic Caching](https://www.redisvl.com/user_guide/llmcache_03.html) is designed to boost the efficiency of applications interacting with LLMs by caching responses based on semantic similarity. For example, when similar user queries are presented to the app, previously cached responses can be used instead of processing the query through the model again, significantly reducing response times and API costs.\n",
    "\n",
    "To do this, use the SemanticCache interface. You can store user queries and response pairs in the semantic cache as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llmcache:942def08d0b47f03895ce7849f25d1d67a5e4311b9c3dcfdf1b0c878acc34f65'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "\n",
    "# Set up the LLM cache\n",
    "llmcache = SemanticCache(\n",
    "    name=\"llmcache\",        # underlying search index name\n",
    "    vectorizer = aoai,      # vectorizer object for embeddings\n",
    "    redis_client = conn,    # Redis client\n",
    "    distance_threshold=0.2  # semantic cache distance threshold\n",
    ")\n",
    " \n",
    "# Cache the question, answer, and arbitrary metadata\n",
    "llmcache.store(\n",
    "    prompt=\"What is the capital city of France?\",\n",
    "    response=\"Paris\",\n",
    "    metadata={\"city\": \"Paris\", \"country\": \"france\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a new query is received, its embedding is compared against those in the semantic cache. If a sufficiently similar embedding is found, the corresponding cached response is served, bypassing the need for another expensive LLM computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for a semantically similar result\n",
    "question = \"What actually is the capital of France?\"\n",
    " \n",
    "llmcache.check(prompt=question)[0]['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect with the `rvl` CLI\n",
    "Use the rvl CLI to inspect the created index and its fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m11:29:43\u001b[0m \u001b[34m[RedisVL]\u001b[0m \u001b[1;30mINFO\u001b[0m   Indices:\n",
      "\u001b[32m11:29:43\u001b[0m \u001b[34m[RedisVL]\u001b[0m \u001b[1;30mINFO\u001b[0m   1. llmcache\n",
      "\u001b[32m11:29:43\u001b[0m \u001b[34m[RedisVL]\u001b[0m \u001b[1;30mINFO\u001b[0m   2. sec-filings\n"
     ]
    }
   ],
   "source": [
    "!rvl index listall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Index Information:\n",
      "╭──────────────┬────────────────┬────────────┬─────────────────┬────────────╮\n",
      "│ Index Name   │ Storage Type   │ Prefixes   │ Index Options   │   Indexing │\n",
      "├──────────────┼────────────────┼────────────┼─────────────────┼────────────┤\n",
      "│ sec-filings  │ HASH           │ ['chunk']  │ []              │          0 │\n",
      "╰──────────────┴────────────────┴────────────┴─────────────────┴────────────╯\n",
      "Index Fields:\n",
      "╭───────────────────┬───────────────────┬─────────┬────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬────────────────┬─────────────────┬────────────────┬────────────────┬────────────────┬─────────────────┬────────────────╮\n",
      "│ Name              │ Attribute         │ Type    │ Field Option   │ Option Value   │ Field Option   │ Option Value   │ Field Option   │   Option Value │ Field Option    │ Option Value   │ Field Option   │   Option Value │ Field Option    │   Option Value │\n",
      "├───────────────────┼───────────────────┼─────────┼────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼────────────────┼─────────────────┼────────────────┼────────────────┼────────────────┼─────────────────┼────────────────┤\n",
      "│ id                │ id                │ TAG     │ SEPARATOR      │ ,              │                │                │                │                │                 │                │                │                │                 │                │\n",
      "│ content           │ content           │ TEXT    │ WEIGHT         │ 1              │                │                │                │                │                 │                │                │                │                 │                │\n",
      "│ company           │ company           │ TAG     │ SEPARATOR      │ ,              │                │                │                │                │                 │                │                │                │                 │                │\n",
      "│ timestamp         │ timestamp         │ NUMERIC │ SORTABLE       │ UNF            │                │                │                │                │                 │                │                │                │                 │                │\n",
      "│ content_embedding │ content_embedding │ VECTOR  │ algorithm      │ HNSW           │ data_type      │ FLOAT32        │ dim            │           1536 │ distance_metric │ COSINE         │ M              │             16 │ ef_construction │            200 │\n",
      "╰───────────────────┴───────────────────┴─────────┴────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴────────────────┴─────────────────┴────────────────┴────────────────┴────────────────┴─────────────────┴────────────────╯\n"
     ]
    }
   ],
   "source": [
    "!rvl index info -i sec-filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "╭─────────────────────────────┬─────────────╮\n",
      "│ Stat Key                    │ Value       │\n",
      "├─────────────────────────────┼─────────────┤\n",
      "│ num_docs                    │ 2           │\n",
      "│ num_terms                   │ 9           │\n",
      "│ max_doc_id                  │ 2           │\n",
      "│ num_records                 │ 17          │\n",
      "│ percent_indexed             │ 1           │\n",
      "│ hash_indexing_failures      │ 0           │\n",
      "│ number_of_uses              │ 4           │\n",
      "│ bytes_per_record_avg        │ 75.5882     │\n",
      "│ doc_table_size_mb           │ 0.000202179 │\n",
      "│ inverted_sz_mb              │ 0.00122547  │\n",
      "│ key_table_size_mb           │ 8.29697e-05 │\n",
      "│ offset_bits_per_record_avg  │ 8           │\n",
      "│ offset_vectors_sz_mb        │ 8.58307e-06 │\n",
      "│ offsets_per_term_avg        │ 0.529412    │\n",
      "│ records_per_doc_avg         │ 8.5         │\n",
      "│ sortable_values_size_mb     │ 0.00030899  │\n",
      "│ total_indexing_time         │ 1.123       │\n",
      "│ total_inverted_index_blocks │ 22          │\n",
      "│ vector_index_sz_mb          │ 6.21588     │\n",
      "╰─────────────────────────────┴─────────────╯\n"
     ]
    }
   ],
   "source": [
    "!rvl stats -i sec-filings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
