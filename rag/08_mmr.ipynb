{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Marginal Relevance (MMR) \n",
    "The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents.\n",
    "\n",
    "When it comes to retrieving documents, the majority of methods will do a similarity metric like cosine similarity, euclidean distance, or dot product. All of these will return documents that are most similar to your query/question.\n",
    "\n",
    "However, what if you want similar documents which are also diverse from each other? That is where Maximum Marginal Relevance (MMR) steps in.\n",
    "\n",
    "The goal is to take into account how similar retrieved documents are to each other when determining which to return. In theory, you should have a well rounded, diverse set of documents.\n",
    "\n",
    "Why is this helpful?\n",
    "- Complex Queries with Multiple Aspects: When a query has several components or aspects, MMR can help retrieve a set of documents that cover all different aspects of the query, rather than just focusing on one.\n",
    "- Avoiding Redundant Information: In cases where the top documents returned by a simple similarity search are very similar to each other, MMR helps avoid redundant information.\n",
    "- Content Summarization: When creating summaries from a large set of documents, MMR can help identify key pieces of information that are both relevant and non-repetitive, which can help answer summarization questions better.\n",
    "- Query Disambiguation: When a query term is ambiguous or has multiple meanings, MMR can retrieve documents that represent the different senses or contexts of the term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load up our keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our LangChain imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's go get our data and chunk it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 1 documents have been split into 28 chunks\n"
     ]
    }
   ],
   "source": [
    "# Loading a single website\n",
    "loader = WebBaseLoader(\"http://www.paulgraham.com/wealth.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split your website into big chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print (f\"Your {len(docs)} documents have been split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll get our embeddings and vectorstore engine going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    ")\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll make two retrievers to compare the outputs with each other:\n",
    "\n",
    "- **Vanilla** - Regular Top K Similarity Search\n",
    "- **MMR** - Do a MMR search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_vanilla = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})\n",
    "retriever_mmr = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go get the docs that come from the vanilla retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_relevant_docs = retriever_vanilla.invoke(\"What is the best way to make and keep wealth?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the docs that come from the MMR retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_relevant_docs = retriever_mmr.invoke(\"What is the best way to make and keep wealth?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long winded function to help compare the two lists together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_list_overlap(list1, list2, content_attr='page_content'):\n",
    "    \"\"\"\n",
    "    Analyze the overlap and uniqueness between two lists of objects using a specified content attribute.\n",
    "\n",
    "    Parameters:\n",
    "    list1 (list): The first list of objects to compare.\n",
    "    list2 (list): The second list of objects to compare.\n",
    "    content_attr (str): The attribute name of the content to use for comparison.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with counts of overlapping, unique to list1, unique to list2 items,\n",
    "          and total counts for each list.\n",
    "    \"\"\"\n",
    "    # Extract unique content attributes from the lists\n",
    "    set1_contents = {getattr(doc, content_attr) for doc in list1}\n",
    "    set2_contents = {getattr(doc, content_attr) for doc in list2}\n",
    "\n",
    "    # Find the number of overlapping content attributes\n",
    "    overlap_contents = set1_contents & set2_contents\n",
    "    overlap_count = len(overlap_contents)\n",
    "\n",
    "    # Find the unique content attributes in each list\n",
    "    unique_to_list1_contents = set1_contents - set2_contents\n",
    "    unique_to_list2_contents = set2_contents - set1_contents\n",
    "    unique_to_list1_count = len(unique_to_list1_contents)\n",
    "    unique_to_list2_count = len(unique_to_list2_contents)\n",
    "\n",
    "    # Use the unique content attributes to retrieve the unique objects\n",
    "    unique_to_list1 = [doc for doc in list1 if getattr(doc, content_attr) in unique_to_list1_contents]\n",
    "    unique_to_list2 = [doc for doc in list2 if getattr(doc, content_attr) in unique_to_list2_contents]\n",
    "\n",
    "    # Count the total number of items in each list\n",
    "    total_list1 = len(list1)\n",
    "    total_list2 = len(list2)\n",
    "\n",
    "    # Return the results in a dictionary\n",
    "    return {\n",
    "        'total_list1': total_list1,\n",
    "        'total_list2': total_list2,\n",
    "        'overlap_count': overlap_count,\n",
    "        'unique_to_list1_count': unique_to_list1_count,\n",
    "        'unique_to_list2_count': unique_to_list2_count,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's actually compare the lists and see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_list1': 8,\n",
       " 'total_list2': 8,\n",
       " 'overlap_count': 7,\n",
       " 'unique_to_list1_count': 1,\n",
       " 'unique_to_list2_count': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_list_overlap(vanilla_relevant_docs, mmr_relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to inspect the 2 MMR docs which are different you would expect they would be more diverse than the ones returned by the vanilla retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
