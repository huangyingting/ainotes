{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import logging\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a single website\n",
    "loader = WebBaseLoader(\"http://www.paulgraham.com/superlinear.html\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split it up\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store your docs/vectors\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the primary applications where superlinear returns are observed?', '2. In what scenarios do superlinear returns typically occur?', '3. Can you list examples of situations that lead to superlinear returns?']\n"
     ]
    }
   ],
   "source": [
    "# Set logging for the queries\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# Your original question\n",
    "question = \"What are the fundamental use cases of superlinear returns?\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fundamental use cases of superlinear returns, as mentioned in the context provided, reduce to two primary causes:\n",
      "\n",
      "1. Exponential Growth: This is when you're working on something that grows exponentially, such as bacterial cultures. In such cases, the growth rate accelerates over time, leading to returns that are disproportionately large compared to the initial investment or effort.\n",
      "\n",
      "2. Thresholds: This involves situations where there is a critical point that must be reached before any significant returns are realized. Once this threshold is crossed, the returns can increase dramatically, often more than proportionally to the additional effort or resources invested.\n",
      "\n",
      "These two causes underpin many scenarios where superlinear returns are observed, from business and technology to social dynamics and natural phenomena.\n"
     ]
    }
   ],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "answer = chain.invoke({\"input_documents\": unique_docs, \"question\":question})\n",
    "print(answer['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. In what scenarios do superlinear returns typically occur?', '2. Can you list the primary applications where superlinear returns are observed?', '3. What are the key situations that lead to superlinear returns?', '4. Could you identify the basic instances where superlinear returns are significant?', '5. What are the core examples of phenomena that demonstrate superlinear returns?']\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You are an AI language model assistant.\n",
    "\n",
    "Your task is to generate 5 different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "\n",
    "By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations  of distance-based similarity search.\n",
    "\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm, prompt=PROMPT\n",
    ")\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
