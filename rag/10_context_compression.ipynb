{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Compression\n",
    "\n",
    "### Overview\n",
    "Contextual Compression is method of the to transform your retrieved documents\n",
    "\n",
    "Contextual Compression refers to the process of extracting information from your retrieved docs that you think will be relevant to your final answer. It's all about increasing the signal-to-noise ratio.\n",
    "\n",
    "Contextual compression works by iterating over your retrieved documents, then passing them through a LLM which will extract information according to context you specify in a prompt.\n",
    "\n",
    "You're compressing your final docs based on context you give it, \"contextual compression\" get it?\n",
    "\n",
    "The key component here is the \"compressor\" which will do our extraction/compressing for us.\n",
    "\n",
    "Say you wanted to know everything about bananas, but you retrieved document talks about apples, oranges, and bananas. The compressor will pull out everything about bananas and then pass it on to your final prompt for a response.\n",
    "\n",
    "### Why is this helpful?\n",
    "The problem with vanilla retrieval is once you chunk your original documents, your chunks may have multiple semantic topics held within them. If you were to pass those multiple topics to the LLM, you may confuse it.\n",
    "\n",
    "Contextual Compression is helpful when you want to try and refine your retrieved documents further.\n",
    "\n",
    "### What are the downsides?\n",
    "You'll be doing an addition API call(s) based on the number of retrieved documents you have. This will increase costs and latency of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by grabbing our keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's import the packages we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and load up some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 1 documents have been split into 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Loading a single website\n",
    "loader = WebBaseLoader(\"http://www.paulgraham.com/superlinear.html\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split your website into big chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print (f\"Your {len(docs)} documents have been split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll get our embeddings and vectorstore ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\"),\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    ")\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to set up our compressor, it's cool that it's a separate object because that means you can use it elsewhere outside this retriever as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vectordb.as_retriever(search_kwargs={\"k\": 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we have our compressor set up, let's try vanilla retrieval first to see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 2 relevant docs\n",
      "gradual improvements in technique, not the discoveries of a few\n",
      "exceptionally learned people.[3]\n",
      "It's not mathematically correct to describe a step function as\n",
      "superlinear, but a step function starting from zero works like a\n",
      "superlinear function when it describes the reward curve for effort\n",
      "by a rational actor. If it starts at zero then the part before the\n",
      "step is below any linearly increasing return, and the part after\n",
      "the step must be above the necessary return at that point or no one\n",
      "would bo\n",
      "Your first document has length: 3920\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = compression_retriever.base_retriever.invoke(\"What do people think is a flaw of capitalism?\")\n",
    "print (f\"You have {len(relevant_docs)} relevant docs\")\n",
    "print (relevant_docs[0].page_content[:500])\n",
    "print (f\"Your first document has length: {len(relevant_docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's see what this same document looks like, but compressed based on the context we give it.\n",
    "\n",
    "We are going to pass a question to the compressor and with that question we will compress the doc. The cool part is this doc will be contextually compressed, meaning the resulting file will only have the information relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Perhaps monopoly or regulation make it hard to compete.\\nPerhaps customers have bad taste or have broken procedures for\\ndeciding what to buy. There are huge swathes of mediocre things\\nthat exist for such reasons.', metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'}),\n",
       " Document(page_content=\"Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer.\", metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressor.compress_documents(documents=relevant_docs, query=\"What do people think is a flaw of capitalism?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great so we had a long document, now we have a shorter document with more dense information. Great for getting rid of the fluff. Let's try it out on our essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What do people think is a flaw of capitalism?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have docs but they are shorter and only contain the information that is relevant to our query.\n",
    "\n",
    "Let's put it in our prompt template again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some people think that the phenomenon of superlinear returns, where the rich get richer, is a flaw of capitalism.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = PROMPT | llm | output_parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": compressed_docs,\n",
    "    \"question\": question\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
