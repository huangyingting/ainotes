{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 15:05:51,487 - 139850290233472 - __init__.py-__init__:538 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mIdentify the next big trend in AI\u001b[00m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to gather the current date to ensure the research report is relevant and timely before identifying the next big trend in AI.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mGet current date\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "2024-11-07\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Researcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```markdown\n",
      "# Research Report: The Next Big Trend in AI (As of November 7, 2024)\n",
      "\n",
      "## Introduction\n",
      "Artificial Intelligence (AI) is evolving at an unprecedented pace, influencing various sectors worldwide. As we look toward the future, several trends are emerging that promise to reshape industries, enhance efficiency, and unlock new capabilities. This report identifies and analyzes the next big trend in AI.\n",
      "\n",
      "## The Next Big Trend: Explainable AI (XAI)\n",
      "\n",
      "### Overview\n",
      "Explainable AI (XAI) refers to methods and techniques in AI that render the workings of AI systems understandable to humans. As AI systems become increasingly complex, the demand for transparency and interpretability has surged, driven by ethical considerations, regulatory requirements, and the necessity for trust in AI systems.\n",
      "\n",
      "### Key Drivers of the Trend\n",
      "1. **Regulatory Pressure**: Governments worldwide are implementing regulations that require companies to ensure their AI systems are transparent and accountable.\n",
      "2. **Ethical Considerations**: There is a growing recognition of the ethical implications of AI. Stakeholders demand clarity on how AI-driven decisions are made, particularly in critical areas like healthcare, law enforcement, and finance.\n",
      "3. **Consumer Trust**: As users become more aware of AI's capabilities and limitations, organizations must provide explanations for AI decisions to build and maintain trust.\n",
      "\n",
      "### Technologies Enabling Explainability\n",
      "- **Interpretable Models**: Simple models that inherently provide insight into their decision-making process, such as decision trees or linear regression.\n",
      "- **Post-hoc Explanation Techniques**: Techniques that analyze complex models after they are trained to offer insights, like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations).\n",
      "- **Visualization Tools**: Tools that help visualize AI decision processes, making them more understandable to users.\n",
      "\n",
      "### Case Studies\n",
      "1. **Healthcare**: Hospitals implementing AI systems for diagnosing diseases are leveraging XAI to help doctors understand AI recommendations, thereby improving patient trust and treatment efficacy.\n",
      "2. **Finance**: Financial institutions are required to provide justifications for lending decisions, prompting the implementation of explainable AI to comply with regulations.\n",
      "\n",
      "### Challenges Ahead\n",
      "While the trend towards explainable AI is promising, several challenges remain:\n",
      "- **Complexity vs. Interpretability**: Balancing the complexity of AI models with the need for interpretability.\n",
      "- **Standardization**: Developing universal standards for explainability that can be adopted across different industries.\n",
      "- **Scalability**: Ensuring that explainable AI techniques can be scaled to meet the demands of large, complex systems.\n",
      "\n",
      "### Conclusion\n",
      "As we move further into the 2020s, explainable AI is set to become a cornerstone of responsible AI deployment. Organizations that prioritize transparency will not only comply with emerging regulations but also harness the benefits of trust and understanding in their AI systems. The convergence of technology, ethics, and regulation suggests that explainable AI will shape the future landscape of artificial intelligence.\n",
      "\n",
      "## References\n",
      "- Doshi-Velez, F., & Kim, P. (2017). Towards a rigorous science of interpretable machine learning.\n",
      "- Gilpin, L. H., et al. (2018). Explaining explanations: An overview of interpretability of machine learning.\n",
      "- European Commission. (2021). White Paper on Artificial Intelligence: A European approach to excellence and trust.\n",
      "\n",
      "```\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrewOutput(raw=\"```markdown\\n# Research Report: The Next Big Trend in AI (As of November 7, 2024)\\n\\n## Introduction\\nArtificial Intelligence (AI) is evolving at an unprecedented pace, influencing various sectors worldwide. As we look toward the future, several trends are emerging that promise to reshape industries, enhance efficiency, and unlock new capabilities. This report identifies and analyzes the next big trend in AI.\\n\\n## The Next Big Trend: Explainable AI (XAI)\\n\\n### Overview\\nExplainable AI (XAI) refers to methods and techniques in AI that render the workings of AI systems understandable to humans. As AI systems become increasingly complex, the demand for transparency and interpretability has surged, driven by ethical considerations, regulatory requirements, and the necessity for trust in AI systems.\\n\\n### Key Drivers of the Trend\\n1. **Regulatory Pressure**: Governments worldwide are implementing regulations that require companies to ensure their AI systems are transparent and accountable.\\n2. **Ethical Considerations**: There is a growing recognition of the ethical implications of AI. Stakeholders demand clarity on how AI-driven decisions are made, particularly in critical areas like healthcare, law enforcement, and finance.\\n3. **Consumer Trust**: As users become more aware of AI's capabilities and limitations, organizations must provide explanations for AI decisions to build and maintain trust.\\n\\n### Technologies Enabling Explainability\\n- **Interpretable Models**: Simple models that inherently provide insight into their decision-making process, such as decision trees or linear regression.\\n- **Post-hoc Explanation Techniques**: Techniques that analyze complex models after they are trained to offer insights, like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations).\\n- **Visualization Tools**: Tools that help visualize AI decision processes, making them more understandable to users.\\n\\n### Case Studies\\n1. **Healthcare**: Hospitals implementing AI systems for diagnosing diseases are leveraging XAI to help doctors understand AI recommendations, thereby improving patient trust and treatment efficacy.\\n2. **Finance**: Financial institutions are required to provide justifications for lending decisions, prompting the implementation of explainable AI to comply with regulations.\\n\\n### Challenges Ahead\\nWhile the trend towards explainable AI is promising, several challenges remain:\\n- **Complexity vs. Interpretability**: Balancing the complexity of AI models with the need for interpretability.\\n- **Standardization**: Developing universal standards for explainability that can be adopted across different industries.\\n- **Scalability**: Ensuring that explainable AI techniques can be scaled to meet the demands of large, complex systems.\\n\\n### Conclusion\\nAs we move further into the 2020s, explainable AI is set to become a cornerstone of responsible AI deployment. Organizations that prioritize transparency will not only comply with emerging regulations but also harness the benefits of trust and understanding in their AI systems. The convergence of technology, ethics, and regulation suggests that explainable AI will shape the future landscape of artificial intelligence.\\n\\n## References\\n- Doshi-Velez, F., & Kim, P. (2017). Towards a rigorous science of interpretable machine learning.\\n- Gilpin, L. H., et al. (2018). Explaining explanations: An overview of interpretability of machine learning.\\n- European Commission. (2021). White Paper on Artificial Intelligence: A European approach to excellence and trust.\\n\\n```\", pydantic=None, json_dict=None, tasks_output=[TaskOutput(description='Identify the next big trend in AI', name=None, expected_output='A research report in markdown format', summary='Identify the next big trend in AI...', raw=\"```markdown\\n# Research Report: The Next Big Trend in AI (As of November 7, 2024)\\n\\n## Introduction\\nArtificial Intelligence (AI) is evolving at an unprecedented pace, influencing various sectors worldwide. As we look toward the future, several trends are emerging that promise to reshape industries, enhance efficiency, and unlock new capabilities. This report identifies and analyzes the next big trend in AI.\\n\\n## The Next Big Trend: Explainable AI (XAI)\\n\\n### Overview\\nExplainable AI (XAI) refers to methods and techniques in AI that render the workings of AI systems understandable to humans. As AI systems become increasingly complex, the demand for transparency and interpretability has surged, driven by ethical considerations, regulatory requirements, and the necessity for trust in AI systems.\\n\\n### Key Drivers of the Trend\\n1. **Regulatory Pressure**: Governments worldwide are implementing regulations that require companies to ensure their AI systems are transparent and accountable.\\n2. **Ethical Considerations**: There is a growing recognition of the ethical implications of AI. Stakeholders demand clarity on how AI-driven decisions are made, particularly in critical areas like healthcare, law enforcement, and finance.\\n3. **Consumer Trust**: As users become more aware of AI's capabilities and limitations, organizations must provide explanations for AI decisions to build and maintain trust.\\n\\n### Technologies Enabling Explainability\\n- **Interpretable Models**: Simple models that inherently provide insight into their decision-making process, such as decision trees or linear regression.\\n- **Post-hoc Explanation Techniques**: Techniques that analyze complex models after they are trained to offer insights, like LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations).\\n- **Visualization Tools**: Tools that help visualize AI decision processes, making them more understandable to users.\\n\\n### Case Studies\\n1. **Healthcare**: Hospitals implementing AI systems for diagnosing diseases are leveraging XAI to help doctors understand AI recommendations, thereby improving patient trust and treatment efficacy.\\n2. **Finance**: Financial institutions are required to provide justifications for lending decisions, prompting the implementation of explainable AI to comply with regulations.\\n\\n### Challenges Ahead\\nWhile the trend towards explainable AI is promising, several challenges remain:\\n- **Complexity vs. Interpretability**: Balancing the complexity of AI models with the need for interpretability.\\n- **Standardization**: Developing universal standards for explainability that can be adopted across different industries.\\n- **Scalability**: Ensuring that explainable AI techniques can be scaled to meet the demands of large, complex systems.\\n\\n### Conclusion\\nAs we move further into the 2020s, explainable AI is set to become a cornerstone of responsible AI deployment. Organizations that prioritize transparency will not only comply with emerging regulations but also harness the benefits of trust and understanding in their AI systems. The convergence of technology, ethics, and regulation suggests that explainable AI will shape the future landscape of artificial intelligence.\\n\\n## References\\n- Doshi-Velez, F., & Kim, P. (2017). Towards a rigorous science of interpretable machine learning.\\n- Gilpin, L. H., et al. (2018). Explaining explanations: An overview of interpretability of machine learning.\\n- European Commission. (2021). White Paper on Artificial Intelligence: A European approach to excellence and trust.\\n\\n```\", pydantic=None, json_dict=None, agent='Senior Researcher', output_format=<OutputFormat.RAW: 'raw'>)], token_usage=UsageMetrics(total_tokens=1354, prompt_tokens=592, completion_tokens=762, successful_requests=2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from crewai import Agent, Task, LLM, Crew, Process\n",
    "from crewai_tools import tool\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@tool(\"Get current date\")\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Get current date.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "llm = LLM(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    model = 'gpt-4o-mini',\n",
    "    azure=True\n",
    ")\n",
    "\n",
    "# Create a researcher agent\n",
    "researcher = Agent(\n",
    "  role='Senior Researcher',\n",
    "  goal='Discover groundbreaking technologies',\n",
    "  backstory='A curious mind fascinated by cutting-edge innovation and the potential to change the world, you know everything about tech.',\n",
    "  tools=[get_current_date],\n",
    "  verbose=True,\n",
    "  llm=llm\n",
    ")\n",
    "\n",
    "# Task for the researcher\n",
    "research_task = Task(\n",
    "  description='Identify the next big trend in AI',\n",
    "  expected_output='A research report in markdown format',\n",
    "  agent=researcher  # Assigning the task to the researcher\n",
    ")\n",
    "\n",
    "# Instantiate your crew\n",
    "tech_crew = Crew(\n",
    "  agents=[researcher],\n",
    "  tasks=[research_task],\n",
    "  verbose=True,\n",
    "  process=Process.sequential  # Tasks will be executed one after the other\n",
    ")\n",
    "\n",
    "# Begin the task execution\n",
    "tech_crew.kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
