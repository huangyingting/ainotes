{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Introduction\n",
        "This notebook provides an idea of how we can leverage Azure AI services to build PoCs. The goal is to provide an understanding on how a Data Scientist/ Cloud Solution Architect can solve a real-world problem into an artificial intelligence solution.\n",
        "\n",
        "### Use case\n",
        "Knowledge bases in enterprises are very common in the industry today and can have extensive number of documents in different categories. Retrieving relevant content based on a user query is a challenging task. This notebook walks through on how leverage Azure OpenAI models to create embeddings from the documents, store in Redis Cache, and leverage vector similarity search to extract text based on query.\n",
        "\n",
        "### Step-by-step process flow\n",
        "\n",
        "1. All files are uploaded to Storage account bucket 'nlppapers'\n",
        "2. For each file, call Form Recognizer to get text contents. (We can use other OCR tools. Form recognizer gives you the flexibility to extract text from complex document templates)\n",
        "3. \\[optional\\] Store contents of each files at .txt in 'nlppapers_text' container\n",
        "4. \\[optional\\] For each file in 'nlppapers_text', call OpenAI.embeddings after chunking\n",
        "5. Store these embeddings in Redis Cache\n",
        "6. Use Vector Similarity Search feature of Redis Cache\n",
        "7. Extract top k similar embeddings based on the query\n",
        "8. Prompt engineer to set-up QnA based on passage (passage being top k similar results extracted from Redis Cache)\n",
        "\n",
        "### Prerequisite - \n",
        "**For demo purposes, run redis locally**\n",
        "\n",
        "docker run -d --name redis --rm -p 6379:6379 redislabs/redismod dislabs/redismod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Installing relevant modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: redis in /home/ythuang/venv/lib/python3.10/site-packages (5.0.1)\n",
            "Requirement already satisfied: openai in /home/ythuang/venv/lib/python3.10/site-packages (1.10.0)\n",
            "Requirement already satisfied: azure-ai-formrecognizer in /home/ythuang/venv/lib/python3.10/site-packages (3.3.2)\n",
            "Requirement already satisfied: azure-storage-blob in /home/ythuang/venv/lib/python3.10/site-packages (12.19.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.2 in /home/ythuang/venv/lib/python3.10/site-packages (from redis) (4.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (2.5.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: sniffio in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ythuang/venv/lib/python3.10/site-packages (from openai) (0.24.1)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-ai-formrecognizer) (0.7.1)\n",
            "Requirement already satisfied: azure-common~=1.1 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-ai-formrecognizer) (1.1.28)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-ai-formrecognizer) (1.29.7)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-storage-blob) (0.6.1)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-storage-blob) (42.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /home/ythuang/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /home/ythuang/venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer) (1.16.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /home/ythuang/venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer) (2.31.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /home/ythuang/venv/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: certifi in /home/ythuang/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/ythuang/venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /home/ythuang/venv/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /home/ythuang/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /home/ythuang/venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
            "Requirement already satisfied: pycparser in /home/ythuang/venv/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/ythuang/venv/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ythuang/venv/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ythuang/venv/lib/python3.10/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-ai-formrecognizer) (2.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/ythuang/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install redis openai azure-ai-formrecognizer azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "embedding_deployment_name = 'text-embedding-ada-002'\n",
        "deployment_name = 'gpt-35-turbo-instruct'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Extracts text using form recognizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1681499953179
        }
      },
      "outputs": [],
      "source": [
        "\"\"\" This code sample shows Prebuilt Read operations with the Azure Form Recognizer client library. \n",
        "We can use other OCR tools. Form recognizer provides flexibility to extract text from complex document templates.\"\"\"\n",
        "\n",
        "import os\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
        "\n",
        "def get_content(document_url):\n",
        "    \"\"\"Returns the text content of the file at the given URL.\"\"\"\n",
        "    print(\"Analyzing\", document_url)\n",
        "\n",
        "    document_analysis_client = DocumentAnalysisClient(\n",
        "        endpoint=os.environ[\"FORM_RECOGNIZER_ENDPOINT\"],\n",
        "        credential=AzureKeyCredential(os.environ[\"FORM_RECOGNIZER_KEY\"]),\n",
        "    )\n",
        "    \n",
        "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
        "            \"prebuilt-read\", document_url)\n",
        "    result = poller.result()\n",
        "    return result.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Process documents\n",
        "Functions to\n",
        "- Creates tuple of document name and authenticated URLs for documents in the given container.\n",
        "- Save processed content into the given container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1681489486971
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobSasPermissions, generate_blob_sas\n",
        "import datetime\n",
        "\n",
        "container_name_documents = 'nlppapers' #raw pdf files\n",
        "container_name_processed = 'nlppapers-text' #text files stored in this container\n",
        "\n",
        "def get_authenticated_urls(container_name):\n",
        "    \"\"\"Returns a list of tuple of (document name, authenticated URLs) for\n",
        "    documents in the given container.\"\"\"\n",
        "\n",
        "    urls = []\n",
        "    # Connect to the storage account\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(\n",
        "        os.environ[\"STORAGE_ACCOUNT_CONNECTION_STRING\"])\n",
        "    container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "    # Iterate over the blobs in the container\n",
        "    blob_list = container_client.list_blobs()\n",
        "    for blob in blob_list:\n",
        "        # Retrieve the URL of the blob\n",
        "        blob_client = container_client.get_blob_client(blob.name)\n",
        "        blob_url = blob_client.url\n",
        "\n",
        "        print(f\"Generating authenticated URL for: {blob.name}\")\n",
        "\n",
        "        blob_sas = generate_blob_sas(\n",
        "            account_name=container_client.account_name,\n",
        "            account_key=container_client.credential.account_key,\n",
        "            container_name=container_name,\n",
        "            blob_name=blob.name,\n",
        "            permission=BlobSasPermissions(read=True),\n",
        "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=1))\n",
        "\n",
        "        authenticated_url = f\"{blob_url}?{blob_sas}\"\n",
        "        urls.append((blob.name, authenticated_url))\n",
        "    return urls\n",
        "\n",
        "\n",
        "def save_content_to_file(container_name, filename, content):\n",
        "    # Connect to the storage account\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(\n",
        "        os.environ[\"STORAGE_ACCOUNT_CONNECTION_STRING\"])\n",
        "    container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "    # Create a blob client using the filename as the name for the blob\n",
        "    blob_client = container_client.get_blob_client(filename)\n",
        "\n",
        "    # Upload the content to the blob\n",
        "    blob_client.upload_blob(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Perform light data cleaning and creating embeddings using text-embedding-ada-002 embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1677630944845
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from openai import AzureOpenAI\n",
        "import math\n",
        "import re\n",
        "\n",
        "# Define the chunk size\n",
        "CHUNK_SIZE = 1000\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
        "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def get_embedding(text, model=embedding_deployment_name): \n",
        "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
        "def normalize_text(s, sep_token = \" \\n \"):\n",
        "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
        "    s = re.sub(r\". ,\",\"\",s)\n",
        "    # remove all instances of multiple spaces\n",
        "    s = s.replace(\"..\",\".\")\n",
        "    s = s.replace(\". .\",\".\")\n",
        "    s = s.replace(\"\\n\", \" \")\n",
        "    s = s.strip()\n",
        "    \n",
        "    return s\n",
        "\n",
        "def create_embeddings(text):\n",
        "    \"\"\"Splits the text into chunks and returns a list of (chunk text, embeddings).\"\"\"\n",
        "    # Calculate the number of chunks\n",
        "    num_chunks = math.ceil(len(text) / CHUNK_SIZE)\n",
        "\n",
        "    # Initialize an empty list to store the embeddings\n",
        "    embeddings = []\n",
        "\n",
        "    # Loop over the chunks of text\n",
        "    for i in range(num_chunks):\n",
        "        start = i * CHUNK_SIZE\n",
        "        end = (i + 1) * CHUNK_SIZE\n",
        "\n",
        "        chunk_text = text[start:end]\n",
        "        embedding = get_embedding(normalize_text(chunk_text), model = embedding_deployment_name)\n",
        "        embeddings.append((chunk_text, embedding))\n",
        "    return embeddings\n",
        "    \n",
        "def create_embeddings_for_query(query):\n",
        "    \"\"\"get embeddings for the given query.\"\"\"\n",
        "    return get_embedding(normalize_text(query), model = embedding_deployment_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Setting up connection to Redis Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1677630948572
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to redis\n"
          ]
        }
      ],
      "source": [
        "from redis import Redis\n",
        "from redis.commands.search.field import VectorField\n",
        "from redis.commands.search.field import TextField\n",
        "from redis.commands.search.field import TagField\n",
        "from redis.commands.search.query import Query\n",
        "from redis.commands.search.result import Result\n",
        "\n",
        "redis_conn = Redis(host = os.getenv(\"REDIS_HOST\"), port = os.getenv(\"REDIS_PORT\"))\n",
        "print ('Connected to redis')\n",
        "# Verify connection to Azure Cache for Redis.\n",
        "redis_conn.ping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create redis index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1677630979690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def create_hnsw_index (redis_conn, vector_field_name, initial_size, vector_dimensions=512, distance_metric='COSINE'):\n",
        "    redis_conn.ft().create_index([\n",
        "        VectorField(vector_field_name, \"HNSW\", {\n",
        "            \"TYPE\": \"FLOAT32\", \n",
        "            \"DIM\": vector_dimensions,\n",
        "            \"DISTANCE_METRIC\": distance_metric,\n",
        "            \"INITIAL_CAP\": initial_size,\n",
        "            }),\n",
        "        TextField(\"document\"),\n",
        "        TextField(\"chunk\"),   \n",
        "        TextField(\"text\"),   \n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Storing the embeddings in redis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1677630985606
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import uuid\n",
        "\n",
        "def save_to_redis(client:Redis, vector_field_name, document_name, chunk_embeddings):\n",
        "    p = client.pipeline(transaction=False)\n",
        "    for index, (chunk, embeddings) in enumerate(chunk_embeddings):   \n",
        "        #hash key\n",
        "        key = str(uuid.uuid4())\n",
        "\n",
        "        #hash values\n",
        "        embedding_binary = np.array(embeddings).astype(np.float32).tobytes()\n",
        "        metadata = {\n",
        "            'document': document_name,\n",
        "            'chunk': str(index),\n",
        "            'text': chunk,\n",
        "            vector_field_name: embedding_binary,\n",
        "        }\n",
        "        # HSET\n",
        "        # print(metadata['document'], str(index), str(embedding_binary))\n",
        "        p.hset(key, mapping=metadata)\n",
        "            \n",
        "    p.execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Get a list of documents from Azure blob storage and their authenticated urls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1680203404887
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating authenticated URL for: 1706.03762.pdf\n",
            "Analyzing https://aaisea.blob.core.windows.net/nlppapers/1706.03762.pdf?se=2024-01-31T12%3A33%3A09Z&sp=r&sv=2023-11-03&sr=b&sig=0M1VelsKM/BngPww8nr1RebB0kU7vkdnkQtwx2/FfVY%3D\n"
          ]
        }
      ],
      "source": [
        "# Get the list of all documents and their authenticated URLs. \n",
        "document_urls = get_authenticated_urls(container_name_documents)\n",
        "\n",
        "documents = []\n",
        "\n",
        "for (document, url) in document_urls:\n",
        "    # Get text contents of each document using Form Analyzer.\n",
        "    content = get_content(url)\n",
        "    # Save contents to Storage account container.\n",
        "    save_content_to_file(container_name_processed, document+\".txt\", content)\n",
        "    # And create embeddings for them.\n",
        "\n",
        "    # list of [(chunk_text, embeddings)]\n",
        "    chunk_embeddings = create_embeddings(content)\n",
        "\n",
        "    documents.append({\n",
        "        'document_name': document,\n",
        "        'chunk_embeddings': chunk_embeddings,\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create embedding index in Redis Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1677631479824
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ITEM_KEYWORD_EMBEDDING_FIELD='embeddings'\n",
        "INITIAL_SIZE=1000\n",
        "TEXT_EMBEDDING_DIMENSION=1536\n",
        "\n",
        "redis_conn.flushall()\n",
        "create_hnsw_index(redis_conn, ITEM_KEYWORD_EMBEDDING_FIELD, INITIAL_SIZE, TEXT_EMBEDDING_DIMENSION, 'COSINE')\n",
        "# Verify the index created\n",
        "redis_conn.ft().info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1677631644439
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving to redis:  1706.03762.pdf\n"
          ]
        }
      ],
      "source": [
        "for document in documents:\n",
        "    print(\"Saving to redis: \", document['document_name'])\n",
        "    save_to_redis(\n",
        "        redis_conn,\n",
        "        ITEM_KEYWORD_EMBEDDING_FIELD,\n",
        "        document['document_name'],\n",
        "        document['chunk_embeddings'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1677631756035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def answer_from_chunk(chunk, query):\n",
        "    prompt = \"\"\"Answer the question: \"{}\"\n",
        "    \n",
        "    Only use the following passage: {}\n",
        "    \"\"\".format(chunk, query)\n",
        "\n",
        "    response = client.completions.create(\n",
        "        model=deployment_name,\n",
        "        prompt=prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1677631697558
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def answer_question(query, topK=1):\n",
        "    query_embeddings = np.array(create_embeddings_for_query(query)).astype(np.float32).tobytes()\n",
        "\n",
        "    #prepare the query\n",
        "    q = Query(f'*=>[KNN {topK} @{ITEM_KEYWORD_EMBEDDING_FIELD} $vec_param AS vector_score]').sort_by('vector_score').paging(0,topK).return_fields('vector_score','document','text').dialect(2)\n",
        "    params_dict = {\"vec_param\": query_embeddings}\n",
        "\n",
        "\n",
        "    #Execute the query\n",
        "    results = redis_conn.ft().search(q, query_params = params_dict)\n",
        "\n",
        "    response = None\n",
        "    # Return the first answer\n",
        "    for chunk in results.docs:\n",
        "        print(chunk.id, chunk.document, chunk.text)\n",
        "        answer = answer_from_chunk(chunk.text, query)\n",
        "        return answer.choices[0]\n",
        "\n",
        "    print(\"Response not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1677631760377
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ebeff5b5-fbf8-4dcd-822d-503c8e616fa8 1706.03762.pdf ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
            "3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "3\n",
            "Scaled Dot-Product Attention\n",
            "1\n",
            "MatMul\n",
            "1\n",
            "SoftMax\n",
            "Multi-Head Attention\n",
            "1\n",
            "Linear\n",
            "Concat\n",
            "Mask (opt.)\n",
            "A\n",
            "Scale\n",
            "1\n",
            "MatMul\n",
            "1\n",
            "1\n",
            "Q K\n",
            "Scaled Dot-Product\n",
            "h\n",
            "Attention\n",
            "Linear\n",
            "V\n",
            "Linear\n",
            "Linear\n",
            "V\n",
            "K\n",
            "Q\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dy. We compute the dot products o\n",
            "\n",
            "Attention is a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values. This ensures that the predictions for position i can depend only on the known outputs at positions less than i. Attention is commonly used in natural language processing and is a key component in many deep learning models.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question('What is attention')\n",
        "print(answer.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1677631773508
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Multi-head attention is a mechanism used in the Transformer model that allows for joint attention to be applied to information from different representation subspaces at different positions. It involves concatenating multiple attention heads and projecting them to obtain final values. This approach is more effective than using a single attention head, as it allows for more diverse and comprehensive information processing. In the Transformer, multi-head attention is used in three different ways, including in \"encoder-decoder attention\" layers where the queries come from the previous decoder layer and the memory keys and values come from the encoder. This allows for effective information transfer between the encoder and decoder layers.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question('What is the multi-head attention')\n",
        "print(answer.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1677631789475
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attention mechanism is a technique used in sequence modeling and transduction models to capture dependencies between input and output sequences without considering their distance. It allows for more efficient modeling of long-range dependencies and has become an integral part of various tasks. The Transformer model architecture relies entirely on attention mechanisms, eliminating the need for recurrent networks and allowing for more parallelization. This has led to significant improvements in translation quality and reduced training time. Other models, such as Extended Neural GPU, ByteNet, and ConvS2S, also aim to reduce sequential computation.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question('What is attention mechanism in detail', topK=2)\n",
        "print(answer.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1677631819454
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attention mechanism is a technique used in sequence modeling and transduction models to model dependencies between input and output sequences without considering their distance. It allows for more parallelization and has been shown to improve model performance in various tasks. The Transformer model architecture relies entirely on attention mechanisms, eschewing recurrence and achieving state-of-the-art results in translation quality with significantly less training time.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question('What is attention mechanism?')\n",
        "print(answer.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1677631837394
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "To train a self-attention model, the first step is to gather a dataset of input-output pairs. This dataset should contain sequences of varying lengths, as the model will need to be able to handle inputs of different sizes. Next, the model is trained using a process called backpropagation, where the model's parameters are adjusted based on the error between the predicted output and the actual output. This process is repeated for multiple epochs until the model's performance on a validation set reaches a satisfactory level. Once the model is trained, it can be used to extrapolate to sequence lengths longer than the ones encountered during training. This is one of the key advantages of self-attention layers, as they are able to handle inputs of any length.\n"
          ]
        }
      ],
      "source": [
        "answer = answer_question('How to train self-attention model?')\n",
        "print(answer.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Conclusion\n",
        "This notebook is designed to showcase how a data scientist can leverage Azure OpenAI service with other Azure services like Form Recognizer, Redis Cache, Azure Blob Storage to create PoCs based on the customer requirements.\n",
        "We can do engineering at various steps to customize this PoC based on the customer requirement, document corpus, and prompt engineering, chunk size etc."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
